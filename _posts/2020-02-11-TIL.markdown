---
layout: post
title:  "TIL how to build a Synthesizer in Swift; how to detect beats with AudioKit"
date:   2020-02-11 19:20:00 +0200
categories: [Swift, Synthesizer, AudioKit, Shazam, CoreML]
---
Bookmarking [Building a Synthesizer in Swift](https://medium.com/better-programming/building-a-synthesizer-in-swift-866cd15b731) which is a follow up of [WWDC 2019 session video 510 What's New in AVAudioEngine](https://developer.apple.com/videos/play/wwdc2019/510/). Post is focusing on how the newly announced `AVAudioSourceNode`  can be used to build a musical synthesizer for iOS. Post references [complete git repo](https://github.com/GrantJEmerson/SwiftSynth) and Apple sample [Building a Signal Generator](https://developer.apple.com/documentation/avfoundation/audio_track_engineering/building_a_signal_generator).

Shortly looked into [git repo of Audio Kit Synth One](https://github.com/AudioKit/AudioKitSynthOne) available in the App Store. Here's [more info about the app](https://audiokitpro.com/synth/).

Googling about beat detection with AudioKit came across post [Exploring music basics with AudioKit](https://swifting.io/blog/2016/06/23/19-exploring-music-basics-with-audiokit/).

[How does Shazam work? Music Recognition Algorithms, Fingerprinting, and Processing](https://www.toptal.com/algorithms/shazam-it-music-processing-fingerprinting-and-recognition).

Interesting post from author of Core ML Survival Guide book [Upsampling in Core ML](https://machinethink.net/blog/coreml-upsampling/).

> Tip: At the end of this post there is a handy cheatsheet that lets you look up which Core ML layer to use with your TensorFlow / Keras / PyTorch models.
